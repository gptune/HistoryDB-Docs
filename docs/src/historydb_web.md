# History DB Web

We provide a public shared database at [https://gptune.lbl.gov](https://gptune.lbl.gov) through [NERSC](https://www.nersc.gov)'s [Science Gateways](https://docs.nersc.gov/services/science-gateways/), where users can upload their performance data or download performance data provided by other users.
This section explains how to use the web interface for sharing performance data.

## User Privilege Levels

To assure provenance and avoid uploading bad data, the repository requires login credentials to submit any data.
Please log-in/sign-up at [https://gptune.lbl.gov/account/login/](https://gptune.lbl.gov/account/login/) to use the history database.
Once the user completes registration, the repository manager(s) will review the registration information.
The user may need to wait for our approval to use all the nice features of the history database repository.

There are multiple accessibility options for each submitted data: publicly available data, private data, and sharing with specific users/groups.
Registered (and approved) users can create specific user groups at [https://gptune.lbl.gov/repo/add-group/](https://gptune.lbl.gov/repo/add-group/).
The group information contains a list of user emails with their roles.
There are two user roles *owner* and *member*.
Users with the *owner* role can invite additional members to the group, but *member* can only view/download data which is accessible by the group.
Also, as shown in the below example, the user can view all the performance data submitted by the user at [https://gptune.lbl.gov/repo/user-dashboard/](https://gptune.lbl.gov/repo/user-dashboard/), and can alter the accessibility of data.

![Managing Accessibility](../static/accessibility.png)

## Downloading Performance Data

To browse and download tuning data (e.g. function evaluation results, surrogate models) from the repository, we provide a web-based dashboard at [https://gptune.lbl.gov/repo/dashboard/](https://gptune.lbl.gov/repo/dashboard/).
In the dashboard, users first need to select a tuning problem from the drop-down menu.
The drop-down menu will show the names of available tuning problems along with their categories, e.g. { name: PDGEQRF, category : ScaLAPACK }.

Once a tuning problem is selected, the dashboard will print all available machine configurations, software configurations, and owner information (i.e. who submitted the data).
The user can check the machine/software/user configuration(s) that match the user's interests.
Then, the dashboard will print a table that contains all the filtered results.
The user can click the *Export JSON* button to export to the [JSON format](../static/overview.md), and download the JSON data and use it for autotuning.

The dashboard prints results based on the user's privilege level.
If the user is using the dashboard without signing in, the user will only be able to access publicly available data.

![Download tuning data](../static/downloading.png)

## Uploading Performance Data

To upload function evaluation results and/or surrogate model data, the user can use an online form at [https://gptune.lbl.gov/repo/upload/](https://gptune.lbl.gov/repo/upload/).
The user first needs to select the tuning problem and the machine used for generating the data.
If the user's tuning problem or machine is not shown in the drop-down menu, the user needs to add the tuning problem or machine information into the repository.
Please refer to the following sections: [Adding Tuning Problems](#adding-tuning-problem) and [Adding Machine Information](#adding-machine-information).

Once the tuning problem and the machine are selected, the user can upload data either by using a JSON data file generated by GPTune or by using a text data in the [JSON format](../static/overview.md).
For the given input data that may contain a large number of function evaluation results and surrogate model data, the shared repository automatically checks if there are duplicated data in the repository, and stores only data which are not in the repository.
The repository also checks if the submitted data match the problem space of the tuning problem and if the submitted data provide all the necessary machine configuration information.
Finally the web will print a message about how many function evaluation results and surrogate model data are added to the repository.

![Adding tuning data](../static/uploading.png)

## Adding Tuning Problems

Before uploading any performance data, users need to define their tuning problems in the shared repository unless the same tuning problem already exists in the repository.
Based on the (well-defined) open tuning problem information, multiple users will be able to run autotuning for the same tuning problem and share performance data.

The list of available tuning problems is shown at [https://gptune.lbl.gov/repo/tuning-problems/](https://gptune.lbl.gov/repo/tuning-problems/).
To add a new tuning problem, the user can use an online form at [https://gptune.lbl.gov/repo/add-tuning-problem/](https://gptune.lbl.gov/repo/add-tuning-problem/).
As shown in the below screen shot, the user needs to define the tuning name and select appropriate category/categories of the tuning problem.
Then, the user needs to define the task space, the tuning parameter space, and the output space.
The user can also define which software and which type of information is needed when submitting performance data.
We obtained a tree-based structure of widely used software packages/tools from the [CK's soft](https://cknowledge.io/c/module/soft) database.
We believe that the provided list can cover many different software settings, but users can contact us at gptune-dev@lbl.gov if there are missing software packages/tools/datasets.

Regarding the tuning problem name, it is possible that different users might want to tune the same program but want to use different tuning options (e.g. different software configurations, different tuning parameter set, etc.).
The user does not need to worry about providing a unique name for the tuning problem name field.
The repository will assign a unique name by combining the user-provided tuning problem name by username and date of submission; hence users can differentiate between tuning problems, tuning the same program with different settings.

![Adding tuning problems](../static/adding_tuning_problem.png)

## Adding Machine Information

Similar to tuning problems, the user needs to define machine information using an online form at [https://gptune.lbl.gov/repo/add-machine/](https://gptune.lbl.gov/repo/add-machine/), unless the user's machine information is already available in the repository.
The available machine list is shown at [https://gptune.lbl.gov/repo/machines/](https://gptune.lbl.gov/repo/machines/).

The user first needs to provide the machine name and the site/institute information.
The user can then select the system model type (e.g. HPC systems manufacturer, cloud service provider, etc.).
Another important data field is to provide the processor types of the machine.
We have built a tree-based processor list from popular (HPC) processor vendors such as Intel, NVIDIA, IBM, AMD, ARM, and etc.
This list can cover almost all processor types in the TOP500 supercomputers, but the user can email us gptune-dev@lbl.gov to suggest modifications/additions to the given data list.
The user can select one processor type (homogeneous system) or multiple processor types (heterogeneous system) that make up the user's machine and provide information about the number of nodes/cores contained in the machine.
The user can finally select the interconnect(s) of the machine and submit the form.
Similar to adding a tuning problem, the repository will assign a unique machine name by combining the machine name by username and date of submission.
In case there are multiple records for the same machine (this is possible as the system can continue to consolidate more system resources), the user can still choose one machine record that best suits the user's tuning configurations.

![Adding machine information](../static/adding_machine.png)


